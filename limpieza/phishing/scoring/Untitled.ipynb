{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4247b829-5640-4504-8e2d-8719ca4e24c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrapidfuzz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fuzz\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# RUTAS Y CONSTANTES (user-style)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m REPO_ROOT \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# dos niveles arriba desde scripts/\u001b[39;00m\n\u001b[1;32m     31\u001b[0m DATA_RAW \u001b[38;5;241m=\u001b[39m REPO_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphishing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m DATA_PROCESSED \u001b[38;5;241m=\u001b[39m REPO_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterim\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphishing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "scripts/scoring_utils.py\n",
    "Versión: 1.0 (v2.6 interna)\n",
    "Autor: Alexis Zapico Fernández\n",
    "\n",
    "Descripción:\n",
    "Script para aplicar el scoring heurístico (v1) a un feed de URLs.\n",
    "Usa la estructura de paths definida por el usuario.\n",
    "\n",
    "Uso (desde la raíz del repo):\n",
    "python scripts/scoring_utils.py\n",
    "o\n",
    "python scripts/scoring_utils.py --input data/raw/phishing/database-phishing.txt --whitelist data/whitelists/spanish_domains.csv\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# -------------------------\n",
    "# RUTAS Y CONSTANTES (user-style)\n",
    "# -------------------------\n",
    "REPO_ROOT = Path(__file__).resolve().parents[1]  # dos niveles arriba desde scripts/\n",
    "DATA_RAW = REPO_ROOT / \"data\" / \"raw\" / \"phishing\"\n",
    "DATA_PROCESSED = REPO_ROOT / \"data\" / \"interim\" / \"phishing\"\n",
    "DATA_INTERIM = REPO_ROOT / \"data\" / \"interim\" / \"phishing\"\n",
    "SCRIPTS_DIR = REPO_ROOT / \"scripts\"\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# Default input file (puedes sobreescribir en CLI)\n",
    "INPUT_FILE = DATA_RAW / \"database-phishing.txt\"\n",
    "\n",
    "# Añadir scripts al path si alguien importa desde notebooks\n",
    "sys.path.append(str(SCRIPTS_DIR))\n",
    "\n",
    "# Default whitelist path (ajustable)\n",
    "DEFAULT_WHITELIST = REPO_ROOT / \"data\" / \"whitelists\" / \"spanish_domains.csv\"\n",
    "\n",
    "# Logging básico\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"scoring_utils\")\n",
    "\n",
    "# -------------------------\n",
    "# FUNCIONES AUXILIARES\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "def load_spanish_whitelist(path: Path):\n",
    "    \"\"\"\n",
    "    Carga whitelist de dominios legítimos. Devuelve lista de strings en minúscula.\n",
    "    Acepta CSV con una columna (dominio) o TXT con un dominio por línea.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        logger.warning(\"Whitelist no encontrada en %s — devolviendo lista vacía\", str(path))\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Intentamos CSV primero\n",
    "        df = pd.read_csv(path, header=0)\n",
    "        # coger la primera columna como lista\n",
    "        col = df.columns[0]\n",
    "        lst = df[col].astype(str).str.strip().str.lower().tolist()\n",
    "        logger.info(\"Whitelist cargada desde CSV (%s) — %d dominios\", str(path), len(lst))\n",
    "        return lst\n",
    "    except Exception:\n",
    "        # fallback: fichero plano (1 dominio por línea)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "            lst = [line.strip().lower() for line in fh if line.strip()]\n",
    "        logger.info(\"Whitelist cargada desde TXT (%s) — %d dominios\", str(path), len(lst))\n",
    "        return lst\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# IMPLEMENTACIÓN DEL SCORING (v1 / baseline)\n",
    "# -------------------------\n",
    "\n",
    "def score_url_v1(url: str, spanish_whitelist: list):\n",
    "    \"\"\"\n",
    "    Scoring baseline (v1) — implementa las reglas descritas en scoring.md.\n",
    "    Devuelve (score:int, signals:str).\n",
    "    \"\"\"\n",
    "    url_low = str(url).lower()\n",
    "    parsed = urlparse(url_low)\n",
    "    netloc = parsed.netloc or ''\n",
    "    path = parsed.path or ''\n",
    "    query = parsed.query or ''\n",
    "    domain_core = netloc.split('.')[-2] if '.' in netloc else netloc\n",
    "\n",
    "    score = 0\n",
    "    signals = []\n",
    "\n",
    "    # --- Listas base ---\n",
    "    POSITIVE_KEYWORDS = ['multa', 'pago', 'verificación', 'cliente', 'acceso', 'seguridad', 'confirmación', 'factura',\n",
    "                         'tarjeta']\n",
    "    SPANISH_MARKERS = ['.es', '+34', '€']\n",
    "    SPANISH_BRANDS = ['santander', 'bbva', 'caixabank', 'ing', 'bankia', 'openbank', 'ionos', 'orange', 'movistar',\n",
    "                      'correos', 'dgt']\n",
    "    SPANISH_HOSTINGS = ['webcindario', 'rf.gd']\n",
    "\n",
    "    GENERIC_SP_TOKENS = [\n",
    "        'servicio', 'soporte', 'atencion', 'cliente', 'usuarios', 'ayuda', 'asistencia',\n",
    "        'cuenta', 'acceso', 'inicio', 'login', 'sesion', 'datos', 'perfil', 'portal',\n",
    "        'seguridad', 'verificacion', 'confirmacion', 'actualizacion', 'validacion', 'auth', 'clave', 'codigo',\n",
    "        'envio', 'entrega', 'paquete', 'pedido', 'multa', 'factura', 'notificacion', 'aviso',\n",
    "        'gob', 'oficial', 'tramite', 'tramites', 'agencia', 'impuestos', 'certificado'\n",
    "    ]\n",
    "\n",
    "    BANKING_TOKENS = [\n",
    "        'banco', 'banca', 'bank', 'banking', 'transferencia', 'tarjeta', 'pin', 'clave', 'codigo', 'validacion',\n",
    "        'firma', 'token', 'sms', 'autenticacion', 'movimientos', 'saldo', 'oficinavirtual', 'bancamovil',\n",
    "        'appbanco', 'bancadigital', 'acceso', 'usuarios', 'verificacion', 'serviciocliente', 'soportecliente'\n",
    "    ]\n",
    "\n",
    "    INSTITUTIONAL_TOKENS = [\n",
    "        'ayuntamiento', 'gob', 'gobierno', 'agencia', 'tramite', 'tramites', 'oficial', 'certificado',\n",
    "        'seg-social', 'catastro', 'impuestos', 'tributos', 'dgt', 'hacienda', 'dni', 'salud', 'sanidad'\n",
    "    ]\n",
    "\n",
    "    PROFESSIONAL_TOKENS = [\n",
    "        'asesoria', 'gestoria', 'abogado', 'despacho', 'consultoria', 'contable', 'laboral', 'fiscal', 'bufete',\n",
    "        'notaria'\n",
    "    ]\n",
    "\n",
    "    ECOMMERCE_TOKENS = [\n",
    "        'pedido', 'pedidos', 'compra', 'compras', 'factura', 'facturas', 'recibo', 'recibos',\n",
    "        'abonado', 'tarifa', 'tarifas', 'servicios', 'renovar', 'renovacion', 'contrato',\n",
    "        'suscripcion', 'envio', 'entrega', 'paquete', 'envios', 'devolucion'\n",
    "    ]\n",
    "\n",
    "    LATAM_TLDS = ['.co', '.mx', '.cl', '.ar', '.br', '.pe', '.ec', '.uy', '.py', '.bo', '.sv', '.hn', '.cr', '.gt',\n",
    "                  '.do']\n",
    "    GLOBAL_TLDS = ['.com', '.app', '.net', '.org', '.io', '.web.app', '.dev']\n",
    "\n",
    "    # --- Reglas principales ---\n",
    "    for kw in POSITIVE_KEYWORDS:\n",
    "        if kw in url_low:\n",
    "            score += 1\n",
    "            signals.append(f'has_kw:{kw}')\n",
    "\n",
    "    for m in SPANISH_MARKERS:\n",
    "        if m in url_low:\n",
    "            if m == '.es':\n",
    "                score += 2\n",
    "            else:\n",
    "                score += 1\n",
    "            signals.append(f'spanish_marker:{m}')\n",
    "\n",
    "    for b in SPANISH_BRANDS:\n",
    "        if b in url_low:\n",
    "            score += 1\n",
    "            signals.append(f'spanish_brand:{b}')\n",
    "\n",
    "    for h in SPANISH_HOSTINGS:\n",
    "        if h in url_low:\n",
    "            score += 2\n",
    "            signals.append(f'spanish_hosting:{h}')\n",
    "\n",
    "    if '.com.es' in url_low:\n",
    "        score += 2\n",
    "        signals.append('tld_combo_com_es')\n",
    "\n",
    "    # --- Semánticas compuestas ---\n",
    "    if '.es' in url_low:\n",
    "        if sum(tok in url_low for tok in GENERIC_SP_TOKENS) >= 2:\n",
    "            score += 3\n",
    "            signals.append('generic_service_combo_es')\n",
    "        if any(tok in url_low for tok in BANKING_TOKENS):\n",
    "            score += 3\n",
    "            signals.append('banking_combo_es')\n",
    "        if any(tok in url_low for tok in INSTITUTIONAL_TOKENS + PROFESSIONAL_TOKENS):\n",
    "            score += 3\n",
    "            signals.append('institutional_professional_es')\n",
    "        if any(tok in url_low for tok in ECOMMERCE_TOKENS):\n",
    "            score += 2\n",
    "            signals.append('ecommerce_combo_es')\n",
    "\n",
    "    # --- Whitelist exacta + fuzzy ---\n",
    "    for legit in spanish_whitelist:\n",
    "        if legit in url_low:\n",
    "            score += 2\n",
    "            signals.append(f'spanish_whitelist_match:{legit}')\n",
    "            break\n",
    "        else:\n",
    "            legit_base = legit.split('.')[0]\n",
    "            sim = fuzz.ratio(domain_core, legit_base)\n",
    "            if sim >= 80:\n",
    "                score += 2\n",
    "                signals.append(f'fuzzy_whitelist_match:{legit}:{sim:.0f}')\n",
    "                break\n",
    "\n",
    "    # --- Recuperación y casos específicos ---\n",
    "    spanish_tokens_for_brand = ['ayuda', 'cliente', 'esapp', 'es', 'spain', 'movil', 'ayuntamiento', 'paqueteria', 'paquete',\n",
    "                                'envio', 'entrega']\n",
    "    if any(b in url_low for b in SPANISH_BRANDS) and any(tok in url_low for tok in spanish_tokens_for_brand):\n",
    "        score += 2\n",
    "        signals.append('brand_plus_spanish_token')\n",
    "\n",
    "    try:\n",
    "        parts = netloc.split('.')\n",
    "        if len(parts) > 2:\n",
    "            subdomain_str = '.'.join(parts[:-2])\n",
    "            if any(b in subdomain_str for b in SPANISH_BRANDS):\n",
    "                score += 2\n",
    "                signals.append('brand_in_subdomain')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    shorteners = ['l.ead.me', 'bit.ly', 't.co', 'tinyurl.com', 'ow.ly', 'is.gd']\n",
    "    if any(s in netloc for s in shorteners):\n",
    "        short_tokens = ['spain', 'es', 'dgt', 'bbva', 'correos', 'ing', 'santander', 'caixabank']\n",
    "        if any(tok in path or tok in query for tok in short_tokens):\n",
    "            score += 2\n",
    "            signals.append('shortener_spain')\n",
    "\n",
    "    try:\n",
    "        has_spanish_brand = any(b in url_low for b in SPANISH_BRANDS)\n",
    "        host_has_global_tld = any(tld in netloc for tld in GLOBAL_TLDS)\n",
    "        host_has_es = '.es' in netloc\n",
    "        if has_spanish_brand and host_has_global_tld and not host_has_es:\n",
    "            score += 1\n",
    "            signals.append('brand_global_tld_boost')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- Penalizaciones LATAM ---\n",
    "    for tld in LATAM_TLDS:\n",
    "        if url_low.endswith(tld) or f\"{tld}/\" in url_low:\n",
    "            score -= 2\n",
    "            signals.append(f'latam_tld:{tld}')\n",
    "\n",
    "    for pk in ['pagamento', 'fatura', 'acesso', 'faturas']:\n",
    "        if pk in url_low:\n",
    "            score -= 2\n",
    "            signals.append(f'pt_kw:{pk}')\n",
    "\n",
    "    for lb in ['banrural', 'pichincha', 'itau', 'bradesco', 'yape', 'daviplata']:\n",
    "        if lb in url_low:\n",
    "            score -= 1\n",
    "            signals.append(f'latam_brand:{lb}')\n",
    "\n",
    "    # --- Fuzzy brand match ---\n",
    "    for brand in SPANISH_BRANDS:\n",
    "        sim = fuzz.ratio(domain_core, brand)\n",
    "        if sim >= 80:\n",
    "            score += 2\n",
    "            signals.append(f'fuzzy_brand_match:{brand}:{sim:.0f}')\n",
    "            break\n",
    "\n",
    "    return score, ';'.join(signals)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# APLICACIÓN MASIVA / WRAPPER\n",
    "# -------------------------\n",
    "def apply_scoring_v1(df: pd.DataFrame, whitelist_path: Path = DEFAULT_WHITELIST) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica score_url_v1 a la columna 'url' del DataFrame.\n",
    "    Devuelve el DataFrame con columnas nuevas: score_total, signals_detected, timestamp, scoring_version.\n",
    "    \"\"\"\n",
    "    whitelist = load_spanish_whitelist(whitelist_path)\n",
    "    logger.info(\"Aplicando scoring_v1 a %d URLs (whitelist=%s)\", len(df), str(whitelist_path))\n",
    "    df = df.copy()\n",
    "    df['score_total'], df['signals_detected'] = zip(*df['url'].apply(lambda x: score_url_v1(x, whitelist)))\n",
    "    df['timestamp'] = datetime.now().isoformat()\n",
    "    df['scoring_version'] = 'v1 (v2.6 interna)'\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# EJECUCIÓN DIRECTA (CLI)\n",
    "# -------------------------\n",
    "def _read_input_file(input_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee input como CSV o TXT (una URL por línea). Devuelve DataFrame con columna 'url'.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    if not input_path.exists():\n",
    "        logger.error(\"Input file no encontrado: %s\", str(input_path))\n",
    "        raise FileNotFoundError(input_path)\n",
    "\n",
    "    # Intentamos leer CSV con encabezado\n",
    "    try:\n",
    "        df = pd.read_csv(input_path)\n",
    "        if 'url' not in df.columns:\n",
    "            # si no tiene columna url, intentamos inferir primera columna\n",
    "            first_col = df.columns[0]\n",
    "            df = df[[first_col]].rename(columns={first_col: 'url'})\n",
    "        return df[['url']].dropna().reset_index(drop=True)\n",
    "    except Exception:\n",
    "        # fallback: fichero plano (una URL por línea)\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "            lines = [l.strip() for l in fh if l.strip()]\n",
    "        return pd.DataFrame({'url': lines})\n",
    "\n",
    "\n",
    "def main(input_file: Path = INPUT_FILE, whitelist_path: Path = DEFAULT_WHITELIST, out_dir: Path = DATA_PROCESSED):\n",
    "    input_file = Path(input_file)\n",
    "    whitelist_path = Path(whitelist_path)\n",
    "    out_dir = Path(out_dir)\n",
    "    # --- Validación de rutas ---\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"❌ Archivo de entrada no encontrado: {input_file}\")\n",
    "\n",
    "if not whitelist_path.exists():\n",
    "    raise FileNotFoundError(f\"❌ Whitelist no encontrada: {whitelist_path}\")\n",
    "\n",
    "if not out_dir.exists():\n",
    "    raise FileNotFoundError(f\"❌ Directorio de salida inexistente: {out_dir}\")\n",
    "\n",
    "\n",
    "    df_in = _read_input_file(input_file)\n",
    "    df_scored = apply_scoring_v1(df_in, whitelist_path=whitelist_path)\n",
    "\n",
    "    # Guardar CSV con timestamp\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_file = out_dir / f\"feed_scored_v1_{ts}.csv\"\n",
    "    df_scored.to_csv(out_file, index=False)\n",
    "    logger.info(\"Scoring completado. Output: %s (rows=%d)\", str(out_file), len(df_scored))\n",
    "    print(f\"✅ Scoring completado. Output: {out_file} (rows={len(df_scored)})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3e4234-9890-45d5-b1bb-5c505d30acd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/phishing-env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phishing-env)",
   "language": "python",
   "name": "phishing-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
