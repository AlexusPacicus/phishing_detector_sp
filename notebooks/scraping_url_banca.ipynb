{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2d4178-481d-4e5d-85ea-4ba30d64dd38",
   "metadata": {},
   "source": [
    "# Scraping de URLs Legítimas de Bancos Españoles\n",
    "\n",
    "**Autor:** Alexis Zapico  \n",
    "**Fecha:** 20/07/2025\n",
    "\n",
    "## Objetivo\n",
    "Obtener un dataset de URLs legítimas de las principales entidades bancarias de España.  \n",
    "El resultado servirá como referencia para la validación y entrenamiento del modelo de detección de phishing.  \n",
    "Este notebook **solo recoge y guarda los datos crudos**; la limpieza y análisis se realizarán en etapas posteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48628783-0377-49b9-af44-b66fe55b58d1",
   "metadata": {},
   "source": [
    "## 3. Bancos a scrapear\n",
    "\n",
    "A continuación se muestran los bancos objetivo del scraping.\n",
    "\n",
    "-  BBVA\n",
    "-  Santander\n",
    "-  CaixaBank\n",
    "-  Sabadell\n",
    "-  Bankinter\n",
    "-  Unicaja Banco\n",
    "-  Ibercaja\n",
    "-  Abanca\n",
    "-  Kutxabank\n",
    "-  Cajamar\n",
    "-  Caja Rural\n",
    "-  Openbank\n",
    "-  EVO Banco\n",
    "-  ING España\n",
    "\n",
    "**Criterio de selección:**  \n",
    "Principales entidades bancarias en España, priorizando aquellas con más usuarios y exposición a ciberataques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21feafd-aa96-4047-a91d-3eb83de2d2d1",
   "metadata": {},
   "source": [
    "### Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b55a1379-3972-471f-a909-7893f36ef6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd, os\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1399ac7-2487-4c5f-98ce-83457660baba",
   "metadata": {},
   "source": [
    "### Diccionario con los bancos a scrapear y sus urls principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "487ed31d-8fd2-4886-b244-57528763b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bancos = {\n",
    "    \"BBVA\": \"https://www.bbva.es\",\n",
    "    \"Santander\": \"https://www.bancosantander.es\",\n",
    "    \"CaixaBank\": \"https://www.caixabank.es\",\n",
    "    \"Sabadell\": \"https://www.bancsabadell.com\",\n",
    "    \"Bankinter\": \"https://www.bankinter.com\",\n",
    "    \"Unicaja Banco\": \"https://www.unicajabanco.es\",\n",
    "    \"Ibercaja\": \"https://www.ibercaja.es\",\n",
    "    \"Abanca\": \"https://www.abanca.com\",\n",
    "    \"Kutxabank\": \"https://www.kutxabank.es\",\n",
    "    \"Cajamar\": \"https://www.cajamar.es\",\n",
    "    \"Caja Rural\": \"https://www.ruralvia.com\",\n",
    "    \"Openbank\": \"https://www.openbank.es\",\n",
    "    \"EVO Banco\": \"https://www.evobanco.com\",\n",
    "    \"ING España\": \"https://www.ing.es\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb200f6d-5b30-4166-aecd-8b007b248a13",
   "metadata": {},
   "source": [
    "## Función para scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d10c2f5b-6704-47f8-9d27-6f29575d2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin, urlparse  # Importa funciones para unir y analizar URLs\n",
    "\n",
    "def obtener_urls(base_url):\n",
    "    \"\"\"\n",
    "    Descarga la página principal del banco y extrae las URLs internas.\n",
    "    Acepta URLs del dominio principal y de sus subdominios (pero no de dominios externos).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Realiza la petición HTTP a la URL base del banco con un tiempo máximo de espera de 10 segundos\n",
    "        response = requests.get(base_url, timeout=10)\n",
    "\n",
    "        # Parsea el HTML recibido para poder analizarlo y extraer enlaces\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Obtiene el dominio principal del banco (por ejemplo, bbva.es)\n",
    "        dominio_banco = urlparse(base_url).netloc\n",
    "\n",
    "        # Elimina 'www.' al inicio si lo hubiera, para igualar subdominios y dominio principal\n",
    "        if dominio_banco.startswith(\"www.\"):\n",
    "            dominio_base = dominio_banco[4:]\n",
    "        else:\n",
    "            dominio_base = dominio_banco\n",
    "\n",
    "        # Set para almacenar URLs únicas\n",
    "        urls = set()\n",
    "\n",
    "        # Itera por todos los enlaces <a href=\"...\">\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']  # Extrae el href del enlace\n",
    "            href = urljoin(base_url, href)  # Convierte relativo a absoluto\n",
    "\n",
    "            # Obtiene el dominio del enlace encontrado\n",
    "            dominio_href = urlparse(href).netloc\n",
    "            if dominio_href.startswith(\"www.\"):\n",
    "                dominio_href_base = dominio_href[4:]\n",
    "            else:\n",
    "                dominio_href_base = dominio_href\n",
    "\n",
    "            # Añade la URL solo si es del dominio principal o subdominio (y no externo)\n",
    "            if dominio_href_base == dominio_base or dominio_href_base.endswith('.' + dominio_base):\n",
    "                urls.add(href)\n",
    "\n",
    "        # Devuelve todas las URLs únicas como lista\n",
    "        return list(urls)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si ocurre algún error, informa por pantalla y devuelve lista vacía\n",
    "        print(f\"Error al acceder a {base_url}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19ca3f-8faa-4527-8348-96b3fd796f80",
   "metadata": {},
   "source": [
    "## Guardamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c668b8d4-088f-4056-a386-73bc753e2244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping banco: BBVA\n",
      "Scraping banco: Santander\n",
      "Scraping banco: CaixaBank\n",
      "Scraping banco: Sabadell\n",
      "Scraping banco: Bankinter\n",
      "Scraping banco: Unicaja Banco\n",
      "Scraping banco: Ibercaja\n",
      "Scraping banco: Abanca\n",
      "Scraping banco: Kutxabank\n",
      "Scraping banco: Cajamar\n",
      "Scraping banco: Caja Rural\n",
      "Scraping banco: Openbank\n",
      "Scraping banco: EVO Banco\n",
      "Scraping banco: ING España\n"
     ]
    }
   ],
   "source": [
    "resultados = []\n",
    "for nombre, url_base in bancos.items():\n",
    "    print(f'Scraping banco: {nombre}')\n",
    "    urls_banco = obtener_urls(url_base) # Devuelve una lsita de urls\n",
    "    for url in urls_banco:\n",
    "        resultados.append({'banco': nombre, 'url': url})\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "02d669d6-ab5d-4cdc-a0c0-205166bc74f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados correctamente en data/raw/urls_legitimas_crudo.csv\n",
      "           banco                                                url\n",
      "0      CaixaBank  https://www.caixabank.es/particular/home/parti...\n",
      "1      CaixaBank  https://www.caixabank.es/particular/general/co...\n",
      "2      CaixaBank  https://www.caixabank.es/particular/general/co...\n",
      "3  Unicaja Banco                    https://uniblog.unicajabanco.es\n",
      "4  Unicaja Banco  https://www.unicajabanco.es/es/particulares/cu...\n"
     ]
    }
   ],
   "source": [
    "# Creamos la carpeta \"raw\", si ya existe, no hace nda\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "\n",
    "\n",
    "# Convertimos la lista de resultados en un DataFrame\n",
    "df_scrap = pd.DataFrame(resultados)\n",
    "# Guardamos el DataFrame en un archivo CSV\n",
    "df_scrap.to_csv('../data/raw/urls_legitimas_crudo.csv', index=False)\n",
    "# Mensaje de confirmación y muestra rápida\n",
    "print(f\"Datos guardados correctamente en data/raw/urls_legitimas_crudo.csv\")\n",
    "print(df_scrap.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507e613-36d0-47b6-a1ba-2298b8c6a120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d35cf40-283f-4c19-8bfb-15e94222779d",
   "metadata": {},
   "source": [
    "## 22/07/2025 - Scraping de bancos pendientes\n",
    "\n",
    "**Objetivo del día:**  \n",
    "Realizar scraping de las páginas oficiales de los siguientes bancos, que aún no tienen URLs registradas en la base de datos del proyecto.\n",
    "\n",
    "**Bancos a procesar:**  \n",
    "- Banco de España  \n",
    "- Bankia (ahora CaixaBank)  \n",
    "- BBK / Laboral Kutxa  \n",
    "- ING Direct  \n",
    "- Santander Consumer Finance  \n",
    "- Banca March  \n",
    "- Targobank\n",
    "\n",
    "**Observaciones iniciales:**  \n",
    "- Registrar el número de URLs obtenidas para cada banco.\n",
    "- Anotar cualquier incidencia técnica (captchas, bloqueos, páginas sin enlaces, etc.).\n",
    "- Actualizar la tabla maestra y el diario de campo al finalizar.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea9eb3-f7b9-4a2d-a8fd-eb882c7d769b",
   "metadata": {},
   "source": [
    "“Se han eliminado de la lista de scraping de URLs legítimas las entidades fusionadas o absorbidas (Bankia, BBK/Laboral Kutxa, ING Direct) para evitar redundancia y ruido. Solo se considerarán para la fase de phishing si detecto que siguen siendo suplantadas.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795cb6ff-62a5-4bd6-b098-14b91c592d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bancos_nuevos ={\n",
    "    \"Banco de España\": \"https://www.bde.es\",\n",
    "    \"Santander Consumer Finance\": \"https://www.santanderconsumer.es\",\n",
    "    \"Banca March\": \"https://www.bancamarch.es\",\n",
    "    \"Targobank\": \"https://www.targobank.es\"\n",
    "}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60eaf93b-2703-49e2-9996-dc78cb9b8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aplicamos la función para obtener las urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b485dd3f-a044-40fd-9895-d6632f20f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScrapeandoBanco de España...\n",
      "Banco de España: 671 URLs obtenidas.\n",
      "ScrapeandoSantander Consumer Finance...\n",
      "Santander Consumer Finance: 0 URLs obtenidas.\n",
      "ScrapeandoBanca March...\n",
      "Banca March: 85 URLs obtenidas.\n",
      "ScrapeandoTargobank...\n",
      "Targobank: 0 URLs obtenidas.\n"
     ]
    }
   ],
   "source": [
    "resultados_nuevos = []\n",
    "for nombre, url in bancos_nuevos.items():\n",
    "    print(f'Scrapeando{nombre}...')\n",
    "    try:\n",
    "        urls_banco = obtener_urls(url)\n",
    "        print(f'{nombre}: {len(urls_banco)} URLs obtenidas.')\n",
    "        for url_encontrada in urls_banco:\n",
    "            resultados_nuevos.append({'banco': nombre, 'url': url_encontrada})\n",
    "    except Exception as e:\n",
    "        print(f'Error en {nombre}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "758f0682-5f41-4f03-a2ba-e822d89349fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Guardamos los resultados en un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "958257e5-111b-462f-a421-e04bfa070a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrap_2 = pd.DataFrame(resultados_nuevos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0a706c3-d229-4549-8355-80ec31269c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Guardamos el dataframe en un csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba9c6cbd-257b-4d82-81ea-df1386fdab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrap_2.to_csv('../data/raw/urls_legitimas_22_07_2025.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phishing-env)",
   "language": "python",
   "name": "phishing-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
