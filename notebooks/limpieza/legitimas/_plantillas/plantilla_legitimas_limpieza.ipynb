{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a028db-bfea-4593-bf18-8f06342570b6",
   "metadata": {},
   "source": [
    "# Limpieza — LEGÍTIMAS (Sector: <SECTOR>)\n",
    "\n",
    "**Objetivo**: Normalizar, validar y deduplicar URLs legítimas para su uso en el baseline.\n",
    "\n",
    "- **Entradas**: `data/raw/legitimas/<sector>/*.csv`\n",
    "- **Salida**: `data/processed/legitimas/<sector>/legitimas_<sector>_limpio.csv`\n",
    "- **Última actualización**: YYYY-MM-DD\n",
    "- **Autor**: Alexis Zapico\n",
    "\n",
    "**Definición de Hecho (DoD)**  \n",
    "1) Carga cruda consolidada.  \n",
    "2) Normalización + validación + deduplicado.  \n",
    "3) Métricas básicas impresas.  \n",
    "4) CSV exportado en `processed`.  \n",
    "5) Log añadido a `docs/daily_log.md`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ad527-1a51-416c-9732-324f6280b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUTAS ===\n",
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path = Path().resolve()):\n",
    "    \"\"\"\n",
    "    Sube hasta 10 niveles buscando algo que parezca la raíz del repo:\n",
    "    carpeta 'data', o '.git', o 'README.md'.\n",
    "    \"\"\"\n",
    "    p = start\n",
    "    for _ in range(10):\n",
    "        if (p / \"data\").exists() or (p / \".git\").exists() or (p / \"README.md\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path().resolve()\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "SECTOR = \"<sector>\"  # <-- cambia aquí al duplicar (banca, cripto, ecommerce, ...)\n",
    "\n",
    "RAW_DIR = DATA_DIR / \"raw\" / \"legitimas\" / SECTOR\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\" / \"legitimas\" / SECTOR\n",
    "OUT_FILE = PROCESSED_DIR / f\"legitimas_{SECTOR}_limpio.csv\"\n",
    "\n",
    "# crea carpetas necesarias\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(REPO_ROOT / \"docs\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce46a69-a7db-4b26-a3d4-e820463ca15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HELPERS ===\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlsplit, urlunsplit, unquote\n",
    "import validators  # pip install validators\n",
    "\n",
    "def es_url_valida(u: str) -> bool:\n",
    "    \"\"\"\n",
    "    True si:\n",
    "    - es str\n",
    "    - no es vacía\n",
    "    - empieza por http/https\n",
    "    - pasa validators.url (sintaxis)\n",
    "    \"\"\"\n",
    "    if not isinstance(u, str):\n",
    "        return False\n",
    "    u = u.strip()\n",
    "    if not u or not u.startswith((\"http://\",\"https://\")):\n",
    "        return False\n",
    "    try:\n",
    "        return bool(validators.url(u))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def normalizar_url(u: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza:\n",
    "    - quita espacios y decodifica %xx\n",
    "    - esquema y host a minúsculas\n",
    "    - reconstruye sin fragment (#...)\n",
    "    \"\"\"\n",
    "    if not isinstance(u, str):\n",
    "        return \"\"\n",
    "    u = unquote(u.strip())\n",
    "    u = re.sub(r\"\\s+\", \"\", u)\n",
    "    try:\n",
    "        sp = urlsplit(u)\n",
    "        scheme = (sp.scheme or \"\").lower()\n",
    "        netloc = (sp.netloc or \"\").lower()\n",
    "        path, query = sp.path or \"\", sp.query or \"\"\n",
    "        return urlunsplit((scheme, netloc, path, query, \"\"))\n",
    "    except Exception:\n",
    "        return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890e06f-d8c3-44e5-8838-25384853b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CARGA CRUDA ROBUSTA ===\n",
    "# Soporta .csv y .CSV, múltiples encodings y columnas diversas para URL\n",
    "CANDIDATES = [\"url\", \"URL\", \"Url\", \"enlace\", \"link\", \"href\"]\n",
    "\n",
    "files = sorted(list(RAW_DIR.glob(\"*.csv\")) + list(RAW_DIR.glob(\"*.CSV\")))\n",
    "print(\"Buscando en:\", RAW_DIR.resolve())\n",
    "print(\"Encontrados:\", [p.name for p in files])\n",
    "assert files, f\"No hay CSV en {RAW_DIR}. Revisa la ruta/nombres.\"\n",
    "\n",
    "def read_csv_tolerant(path: Path):\n",
    "    \"\"\"Prueba varios encodings; salta líneas corruptas sin parar el flujo.\"\"\"\n",
    "    for args in (\n",
    "        dict(),  # por defecto\n",
    "        dict(encoding=\"utf-8\", engine=\"python\", on_bad_lines=\"skip\"),\n",
    "        dict(encoding=\"latin-1\", engine=\"python\", on_bad_lines=\"skip\"),\n",
    "    ):\n",
    "        try:\n",
    "            return pd.read_csv(path, **args)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(f\"No se pudo leer {path.name} con los encodings probados.\")\n",
    "\n",
    "dfs, per_file_counts, skipped = [], [], []\n",
    "for f in files:\n",
    "    try:\n",
    "        d = read_csv_tolerant(f)\n",
    "        col_url = next((c for c in CANDIDATES if c in d.columns), None)\n",
    "        if not col_url:\n",
    "            raise KeyError(f\"{f.name} sin columna URL reconocida {CANDIDATES}\")\n",
    "        d = d[[col_url]].rename(columns={col_url: \"url\"}).copy()\n",
    "        d[\"__source_file\"] = f.name\n",
    "        dfs.append(d)\n",
    "        per_file_counts.append((f.name, len(d)))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Saltando {f.name}: {e}\")\n",
    "        skipped.append((f.name, str(e)))\n",
    "\n",
    "assert dfs, \"Ningún CSV válido. Revisa los [WARN] y corrige.\"\n",
    "df_raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Archivos leídos:\")\n",
    "for n, k in per_file_counts:\n",
    "    print(f\" - {n}: {k} filas\")\n",
    "print(\"TOTAL filas crudas:\", len(df_raw))\n",
    "df_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b604ba-8b33-4d96-925d-9f5f80aae0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LIMPIEZA ===\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Normaliza URL (espacios, %xx, esquema/host)\n",
    "df[\"url\"] = df[\"url\"].map(normalizar_url)\n",
    "\n",
    "# Valida URL (http/https + sintaxis ok)\n",
    "df = df[df[\"url\"].map(es_url_valida)]\n",
    "\n",
    "# Quita nulos explícitos\n",
    "df = df.dropna(subset=[\"url\"])\n",
    "\n",
    "# Dedup exacto por URL\n",
    "df = df.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Filas tras limpieza:\", len(df))\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7fc1ac-c250-41f3-b73f-8684b36a8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MÉTRICAS RÁPIDAS ===\n",
    "resumen = {\n",
    "    \"filas_crudas\": len(df_raw),\n",
    "    \"filas_limpias\": len(df),\n",
    "    \"%https\": round(df[\"url\"].str.startswith(\"https://\").mean()*100, 2),\n",
    "    \"longitud_media\": round(df[\"url\"].str.len().mean(), 2),\n",
    "}\n",
    "resumen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21177029-cb8b-4277-9823-9d967ae757ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[\"url\"].str.extract(r\"^https?://([^/]+)/\", expand=False).str.lower()\n",
    "tmp.value_counts().head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54683779-ede0-4415-96a3-62f5fb395674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPORT ===\n",
    "df.to_csv(OUT_FILE, index=False)\n",
    "print(\"Guardado en:\", OUT_FILE)\n",
    "\n",
    "# === LOG (docs/daily_log.md) ===\n",
    "log_line = (\n",
    "    f\"{datetime.now():%Y-%m-%d} | limpieza_legitimas_{SECTOR} | \"\n",
    "    f\"crudo={len(df_raw)} | limpio={len(df)} | out={OUT_FILE}\\n\"\n",
    ")\n",
    "with open(REPO_ROOT / \"docs\" / \"daily_log.md\", \"a\") as f:\n",
    "    f.write(log_line)\n",
    "\n",
    "print(\"Log añadido:\", log_line.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
