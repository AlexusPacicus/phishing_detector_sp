{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d280d0-4bc5-4f82-bd4f-ec8a105fc280",
   "metadata": {},
   "source": [
    "# Scraping de URLs Legítimas – Sector SaaS / Cloud / Correo\n",
    "\n",
    "**Objetivo:**  \n",
    "## 1. Objetivo\n",
    "\n",
    "Documentar el proceso de recopilación de URLs legítimas de los principales servicios SaaS, cloud y correo electrónico, para construir el dataset de entrenamiento del modelo de detección de phishing.\n",
    "\n",
    "> Nota: En esta fase inicial, se replicó la metodología básica usada en banca, sin aplicar adaptaciones específicas para el sector SaaS/Cloud.\n",
    "\n",
    " En esta fase se documentan:\n",
    "- Las empresas objetivo,\n",
    "- las URLs relevantes recopiladas (login, acceso, recuperación, etc.),\n",
    "- los problemas encontrados,\n",
    "- y los próximos pasos identificados.\n",
    "\n",
    "La metodología empleada es scraping básico sobre la home/login, sin crawling profundo ni Selenium.  \n",
    "Las mejoras y adaptaciones específicas se valorarán tras el análisis de resultados de esta primera extracción.\n",
    "\n",
    "## 1. Lista de empresas/servicios objetivo\n",
    "\n",
    "| Empresa/Servicio | URL Principal                       | Sector/Tipo    | Observaciones             |\n",
    "|------------------|-------------------------------------|----------------|---------------------------|\n",
    "| Microsoft        | https://login.microsoftonline.com    | Cloud/Correo   | Top mundial phishing      |\n",
    "| Google           | https://accounts.google.com          | Cloud/Correo   | Top mundial phishing      |\n",
    "| Apple            | https://appleid.apple.com            | Cloud/Correo   | Top mundial phishing      |\n",
    "| Dropbox          | https://www.dropbox.com/login        | Cloud          |                           |\n",
    "| Adobe            | https://account.adobe.com            | SaaS           |                           |\n",
    "| Zoom             | https://zoom.us/signin               | SaaS           |                           |\n",
    "| Slack            | https://slack.com/signin             | SaaS           |                           |\n",
    "| DocuSign         | https://account.docusign.com         | SaaS           |                           |\n",
    "| TeamViewer       | https://login.teamviewer.com         | SaaS           |                           |\n",
    "| Atlassian        | https://id.atlassian.com             | SaaS           |                           |\n",
    "| Box              | https://account.box.com/login        | Cloud          |                           |\n",
    "| Cisco Webex      | https://signin.webex.com             | SaaS           |                           |\n",
    "| Zoho             | https://accounts.zoho.com            | SaaS           |                           |\n",
    "| GoToMeeting      | https://signin.logmeininc.com        | SaaS           |                           |\n",
    "| ProtonMail       | https://mail.proton.me/login         | Correo         |                           |\n",
    "| Fastmail         | https://www.fastmail.com/login       | Correo         |                           |\n",
    "| Mega             | https://mega.nz/login                | Cloud          |                           |\n",
    "| Yandex Mail      | https://passport.yandex.com          | Correo         |                           |\n",
    "| iCloud           | https://www.icloud.com/login         | Cloud/Correo   |                           |\n",
    "| OVHcloud         | https://www.ovh.com/auth/            | Cloud          |                           |\n",
    "\n",
    "\n",
    "## 2. Metodología\n",
    "\n",
    "- **Fase 1:** Scraping básico de la página principal (home/login) de cada servicio, extrayendo únicamente los enlaces internos visibles sin interacción avanzada.\n",
    "- **Fase 2:** Registro y documentación de problemas encontrados durante el scraping (por ejemplo, páginas minimalistas, enlaces limitados, detección de bots).\n",
    "- **Fase 3:** Análisis de resultados obtenidos y planteamiento de mejoras para futuras fases (evaluar la necesidad de crawling profundo o el uso de Selenium en servicios con baja cobertura de URLs).\n",
    "\n",
    "\n",
    "## 3. Resultados iniciales\n",
    "\n",
    "- **Total de URLs obtenidas:** 72\n",
    "- **Empresas cubiertas:** 9\n",
    "- **Problemas detectados:**  \n",
    "  - Algunas webs (ej. Google, Zoom, Mega, iCloud) presentan una home muy minimalista, con pocos enlaces útiles accesibles.\n",
    "  - En la mayoría de casos no se detectaron bloqueos activos ni captchas, pero sí una clara limitación en la profundidad de enlaces recogidos.\n",
    "- **Incidencias:**  \n",
    "  - El dataset resultante es muy desbalanceado: más del 80% de las URLs provienen de Box y Slack, el resto de empresas aportan muy pocos enlaces útiles.\n",
    "  - Posible inclusión de enlaces poco relevantes (ayuda, términos de servicio, etc.) debido a la metodología básica empleada.\n",
    "\n",
    "\n",
    "## 4. Próximos pasos\n",
    "\n",
    "- Filtrar URLs realmente relevantes para phishing.\n",
    "- Valorar crawling profundo y uso de Selenium en webs con baja cobertura.\n",
    "- Ampliar y revisar la lista de empresas en siguientes iteraciones.\n",
    "- Preparar los datos para limpieza y análisis.\n",
    "\n",
    "\n",
    "## 5. Registro de problemas y soluciones\n",
    "\n",
    "- Enlaces útiles limitados en Google, Zoom, Mega e iCloud (pendiente de crawling/Selenium).\n",
    "- Box y Slack aportan muchas URLs pero con mezcla de rutas poco relevantes (pendiente de filtrado).\n",
    "- Sin bloqueos ni captchas detectados en esta fase.\n",
    "\n",
    "\n",
    "## 6. Código y logs relevantes\n",
    "\n",
    "- Notebook principal: `notebooks/scraping_saas.ipynb`\n",
    "- Scripts de scraping y crawling: (añadir enlaces si los separas en otros archivos)\n",
    "- Logs de ejecución: (añadir referencias o archivos si registras errores o incidencias específicas)\n",
    "- Dataset generado: `data/raw/saas_legitimas_crudo.csv`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0387980-6b2b-4f22-8fd2-7440a96e55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd, os\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0eaef-9ac8-413e-9c33-940c6b841c89",
   "metadata": {},
   "source": [
    "## Lista inicial de empresas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2a6a9a-184f-49e0-9c83-23070814041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "saas_empresas = {\n",
    "    \"Microsoft\": \"https://login.microsoftonline.com\",\n",
    "    \"Google\": \"https://accounts.google.com\",\n",
    "    \"Apple\": \"https://appleid.apple.com\",\n",
    "    \"Dropbox\": \"https://www.dropbox.com/login\",\n",
    "    \"Adobe\": \"https://account.adobe.com\",\n",
    "    \"Zoom\": \"https://zoom.us/signin\",\n",
    "    \"Slack\": \"https://slack.com/signin\",\n",
    "    \"DocuSign\": \"https://account.docusign.com\",\n",
    "    \"TeamViewer\": \"https://login.teamviewer.com\",\n",
    "    \"Atlassian\": \"https://id.atlassian.com\",\n",
    "    \"Box\": \"https://account.box.com/login\",\n",
    "    \"Cisco Webex\": \"https://signin.webex.com\",\n",
    "    \"Zoho\": \"https://accounts.zoho.com\",\n",
    "    \"GoToMeeting\": \"https://signin.logmeininc.com\",\n",
    "    \"ProtonMail\": \"https://mail.proton.me/login\",\n",
    "    \"Fastmail\": \"https://www.fastmail.com/login\",\n",
    "    \"Mega\": \"https://mega.nz/login\",\n",
    "    \"Yandex Mail\": \"https://passport.yandex.com\",\n",
    "    \"iCloud\": \"https://www.icloud.com/login\",\n",
    "    \"OVHcloud\": \"https://www.ovh.com/auth/\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d1974-b2c0-4bc3-9e23-ebe5c5fe3d7e",
   "metadata": {},
   "source": [
    "## Función para scrapear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8991a9-aac1-41ad-8b93-3bbb9a560423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_urls(base_url, delay=1):\n",
    "    \"\"\"\n",
    "Descarga la página principal y extrae las URLs internas\n",
    "Acepta URLs del dominio principal y subdominios\n",
    "\"\"\"\n",
    "    try:\n",
    "        # Realiza la petición HTTP a la URL de la enpresa con 10s máximos de espera\n",
    "        response = requests.get(base_url, timeout=10)\n",
    "        \n",
    "        # Parsea el HTML recibido\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Obtiene el dominio principal\n",
    "        dominio_empresa = urlparse(base_url).netloc\n",
    "\n",
    "        # Elimina \"www.\" al inicio para igualar subdominios y dominio principal\n",
    "\n",
    "        if dominio_empresa.startswith('www.'):\n",
    "            dominio_base = dominio_empresa[4:]\n",
    "        else:\n",
    "            dominio_base = dominio_empresa\n",
    "\n",
    "        # Set para almacenar URLs únicas\n",
    "        urls = set()\n",
    "\n",
    "        # Itera por todos los enlaces (a, href)\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href'] # Extrae el href del enlace\n",
    "            href = urljoin(base_url, href) #Convierte relativo a absoluto\n",
    "\n",
    "            #Obtiene el dominio del enlace\n",
    "            dominio_href = urlparse(href).netloc\n",
    "            if dominio_href.startswith('www.'):\n",
    "                dominio_href_base = dominio_href[4:]\n",
    "            else:\n",
    "                dominio_href_base = dominio_href\n",
    "\n",
    "            # Añade la URL solo si es del dominio principal o subdomino\n",
    "            if dominio_href_base == dominio_base or dominio_href_base.endswith('.' + dominio_base):\n",
    "                urls.add(href)\n",
    "\n",
    "        # Devuelve las URLs únicas como lista\n",
    "        return list(urls)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error al acceder a {base_url}: {e}')\n",
    "        return []\n",
    " \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8cadf-9ef1-40d4-8807-e0fc8b7c3aa6",
   "metadata": {},
   "source": [
    "## Guardamos los resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceec64d3-08e3-43ae-9893-203e17fe065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping empresa: Microsoft\n",
      "Scraping empresa: Google\n",
      "Scraping empresa: Apple\n",
      "Scraping empresa: Dropbox\n",
      "Scraping empresa: Adobe\n",
      "Scraping empresa: Zoom\n",
      "Scraping empresa: Slack\n",
      "Scraping empresa: DocuSign\n",
      "Scraping empresa: TeamViewer\n",
      "Scraping empresa: Atlassian\n",
      "Scraping empresa: Box\n",
      "Scraping empresa: Cisco Webex\n",
      "Scraping empresa: Zoho\n",
      "Scraping empresa: GoToMeeting\n",
      "Error al acceder a https://signin.logmeininc.com: HTTPSConnectionPool(host='signin.logmeininc.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x11e44c580>: Failed to resolve 'signin.logmeininc.com' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "Scraping empresa: ProtonMail\n",
      "Scraping empresa: Fastmail\n",
      "Scraping empresa: Mega\n",
      "Scraping empresa: Yandex Mail\n",
      "Scraping empresa: iCloud\n",
      "Scraping empresa: OVHcloud\n"
     ]
    }
   ],
   "source": [
    "resultados = []\n",
    "for nombre, url_base in saas_empresas.items():\n",
    "    print(f'Scraping empresa: {nombre}')\n",
    "    urls_empresa = obtener_urls(url_base) \n",
    "    for url in urls_empresa:\n",
    "        resultados.append({'empresa': nombre, 'url': url})\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa038c45-fc5c-4a9d-b0da-20ab15efa7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertirmos los resultados en un DataFrame\n",
    "df_scrap = pd.DataFrame(resultados)\n",
    "#Guardamos el DataFrame en un archivo CSV\n",
    "df_scrap.to_csv('../data/raw/saas_legitimas_crudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68db3e2f-fe91-4e0b-9e0b-cb2275659f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empresa\n",
      "Box            30\n",
      "Slack          28\n",
      "Google          4\n",
      "Zoom            3\n",
      "Fastmail        2\n",
      "OVHcloud        2\n",
      "Mega            1\n",
      "Yandex Mail     1\n",
      "iCloud          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_scrap['empresa'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12f2ac-a8de-461e-ab6f-3b3922c95bd3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Resumen:**\n",
    "- Se han recolectado 72 URLs de 9 empresas del sector SaaS/Cloud/Correo.\n",
    "- El 80% de las URLs pertenecen a Box y Slack, lo que muestra un dataset muy desbalanceado.\n",
    "- El resto de empresas aportan solo 1-4 URLs cada una, probablemente por falta de enlaces internos o restricciones antirrobots.\n",
    "- No hay duplicados exactos.\n",
    "- Las rutas cubren login, ayuda, TOS, accesibilidad, etc. De cara a entrenamiento de modelos de phishing, sería recomendable filtrar por relevancia (`login`, `signin`, `reset`, etc.).\n",
    "\n",
    "**Limitaciones detectadas:**\n",
    "- Baja diversidad de empresas.\n",
    "- Posible inclusión de rutas poco útiles (TOS, ayuda, etc.).\n",
    "- No se han aplicado técnicas de crawling avanzado ni Selenium.\n",
    "\n",
    "**Próximos pasos propuestos:**\n",
    "- Filtrado y priorización de URLs relevantes para phishing.\n",
    "- Reintentar scraping/crawling más profundo en empresas con baja cobertura.\n",
    "- Evaluar uso de Selenium para webs que bloquean scraping básico.\n",
    "\n",
    "**Checkpoint documentado:**\n",
    "- Fase de scraping/crawling básico cerrada.\n",
    "- Listo para iterar limpieza y crawling avanzado en segunda vuelta.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phishing-env)",
   "language": "python",
   "name": "phishing-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
