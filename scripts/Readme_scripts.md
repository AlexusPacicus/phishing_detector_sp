# üõ†Ô∏è Scripts de Automatizaci√≥n y Scraping de Feeds de Phishing

Esta carpeta contiene los **scripts profesionales** que automatizan la recolecci√≥n, limpieza y almacenamiento de feeds de phishing para su an√°lisis posterior.

---

## üìú ¬øQu√© hace cada script?

- **aut_phishtank.py**  
  Descarga, limpia y guarda el feed CSV de PhishTank, deduplicando y a√±adiendo metadatos para trazabilidad.

- **aut_haus.py**  
  Automatiza la descarga y limpieza del feed CSV de URLhaus, filtrando l√≠neas corruptas y asegurando un dataset usable.

- **automatizacion_openphish.py**  
  Descarga el feed TXT de OpenPhish (lista de URLs), lo transforma en CSV, elimina duplicados y a√±ade metadatos.

- **aut_phishstats.py**  
  Procesa el feed JSON de PhishStats, normaliza, deduplica y enriquece los datos con metadatos.

- **update_tweetfeed.py**  
  Automatiza la descarga y procesamiento del repositorio TweetFeed para obtener URLs phishing del √∫ltimo a√±o.  
  - Clona o actualiza el repositorio local.  
  - Filtra URLs y elimina duplicados del archivo `year.csv`.  
  - A√±ade timestamp de procesamiento.  
  - Guarda el resultado con nombre √∫nico con fecha/hora.  
  - Registra todo con logging rotatorio para seguimiento.  

---

## ‚öôÔ∏è L√≥gica com√∫n de los scripts

- Descarga el feed correspondiente desde la fuente oficial o repositorio Git.  
- Limpia y valida la estructura de los datos (eliminando duplicados, l√≠neas corruptas o vac√≠as).  
- A√±ade columnas est√°ndar:  
  - `fuente`: nombre de la fuente.  
  - `fecha_hora_recoleccion`: timestamp de la ejecuci√≥n.  
- Guarda el resultado en la carpeta `../data/raw/phishing/` con nombre √∫nico por fecha y hora.  
- Registra toda la actividad (intentos, √©xitos, errores) en un archivo rotativo de logs en `../logs/`.

---

## üö¶ Ejecuci√≥n

Lanza cualquier script desde la terminal, dentro de la carpeta `/scripts` o desde la ra√≠z del proyecto:

```bash
python update_tweetfeed.py
