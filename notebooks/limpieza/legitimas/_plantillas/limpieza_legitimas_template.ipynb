{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d853bc7f-80eb-458c-9994-03cbc33fe1be",
   "metadata": {},
   "source": [
    "# Limpieza — LEGÍTIMAS (Sector: )\n",
    "\n",
    "**Objetivo**: normalizar, deduplicar y exportar URLs legítimas para su uso en el baseline.\n",
    "\n",
    "- **Entradas**: `raw/legitimas/<sector>/*.csv`\n",
    "- **Salida**: `processed/legitimas/<sector>/legitimas_<sector>_limpio.csv`\n",
    "- **Última actualización**: 2025-08-18\n",
    "- **Autor**: Alexis Zapico\n",
    "\n",
    "**Definición de Hecho (DoD)**\n",
    "1) Carga CSV crudo del sector desde `raw/legitimas/<sector/`.\n",
    "2) Filtra nulos y URLs inválidas.\n",
    "3) Deduplica por `url`.\n",
    "4) Calcula métricas básicas de validación.\n",
    "5) Exporta a `processed/legitimas/<sector/legitimas_<sector>_limpio.csv`.\n",
    "6) Escribe log en `docs/daily_log.md`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff54b363-e56b-42b6-9037-1253c07f24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== LIBRERÍAS ====\n",
    "import os, re, glob                        # librerías estándar: sistema de archivos, regex y búsqueda de archivos\n",
    "import pandas as pd                        # pandas: análisis y manipulación de datos\n",
    "from datetime import datetime              # para poner fecha y hora en logs\n",
    "from urllib.parse import urlsplit, urlunsplit, unquote  # utilidades para parsear y reconstruir URLs\n",
    "from pathlib import Path                   # manejo de rutas multiplataforma\n",
    "import validators                          # librería externa para validar URLs sintácticamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71025119-1b70-470e-afba-5d999bec28bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_ROOT: /Users/test/Desktop/phishing-detector\n",
      "RAW_DIR: /Users/test/Desktop/phishing-detector/data/raw/legitimas/banca\n",
      "PROCESSED_DIR: /Users/test/Desktop/phishing-detector/data/processed/legitimas/banca\n"
     ]
    }
   ],
   "source": [
    "def find_repo_root(start: Path = Path().resolve()):\n",
    "    \"\"\"Sube hasta 10 niveles buscando un directorio que parezca raíz de repo (data/.git/README.md).\"\"\"\n",
    "    p = start\n",
    "    for _ in range(10):\n",
    "        if (p / \"data\").exists() or (p / \".git\").exists() or (p / \"README.md\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path().resolve()\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "SECTOR = \"<sector>\"  \n",
    "\n",
    "RAW_DIR = DATA_DIR / \"raw\" / \"legitimas\" / SECTOR\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\" / \"legitimas\" / SECTOR\n",
    "OUT_FILE = PROCESSED_DIR / f\"legitimas_{SECTOR}_limpio.csv\"\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(REPO_ROOT / \"docs\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3a35e1-cb9b-4ac1-8693-ede9076bd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_url_valida(u: str) -> bool:\n",
    "    if not isinstance(u, str): \n",
    "        return False\n",
    "    u = u.strip()\n",
    "    if not u or not u.startswith((\"http://\",\"https://\")):\n",
    "        return False\n",
    "    try:\n",
    "        return bool(validators.url(u))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def normalizar_url(u: str) -> str:\n",
    "    if not isinstance(u, str): \n",
    "        return \"\"\n",
    "    u = unquote(u.strip())\n",
    "    u = re.sub(r\"\\s+\", \"\", u)\n",
    "    try:\n",
    "        sp = urlsplit(u)\n",
    "        scheme = (sp.scheme or \"\").lower()\n",
    "        netloc = (sp.netloc or \"\").lower()\n",
    "        path, query = sp.path or \"\", sp.query or \"\"\n",
    "        return urlunsplit((scheme, netloc, path, query, \"\"))\n",
    "    except Exception:\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c486aa4-9931-41d0-b32e-c8648f2c289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando en: /Users/test/Desktop/phishing-detector/data/raw/legitimas/banca\n",
      "Encontrados: ['banca_legitimas.csv', 'banca_legitimas_2.csv', 'legitimas_banca.csv']\n",
      "[WARN] Saltando legitimas_banca.csv: \"legitimas_banca.csv sin columna de URL reconocida ['url', 'URL', 'Url', 'enlace', 'link', 'href']\"\n",
      "Archivos leídos:\n",
      " - banca_legitimas.csv: 373 filas\n",
      " - banca_legitimas_2.csv: 756 filas\n",
      "TOTAL filas crudas: 1129\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>__source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.caixabank.es/particular/home/parti...</td>\n",
       "      <td>banca_legitimas.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.caixabank.es/particular/general/co...</td>\n",
       "      <td>banca_legitimas.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.caixabank.es/particular/general/co...</td>\n",
       "      <td>banca_legitimas.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        __source_file\n",
       "0  https://www.caixabank.es/particular/home/parti...  banca_legitimas.csv\n",
       "1  https://www.caixabank.es/particular/general/co...  banca_legitimas.csv\n",
       "2  https://www.caixabank.es/particular/general/co...  banca_legitimas.csv"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === CARGA CRUDA ROBUSTA (banca) ===\n",
    "CANDIDATES = [\"url\", \"URL\", \"Url\", \"enlace\", \"link\", \"href\"]\n",
    "\n",
    "# Busca .csv y .CSV\n",
    "files = sorted(list(RAW_DIR.glob(\"*.csv\")) + list(RAW_DIR.glob(\"*.CSV\")))\n",
    "print(\"Buscando en:\", RAW_DIR.resolve())\n",
    "print(\"Encontrados:\", [p.name for p in files])\n",
    "assert files, f\"No hay CSV en {RAW_DIR}. Revisa la ruta y los nombres.\"\n",
    "\n",
    "def read_csv_tolerant(path: Path):\n",
    "    for args in (\n",
    "        dict(),  # por defecto\n",
    "        dict(encoding=\"utf-8\", engine=\"python\", on_bad_lines=\"skip\"),\n",
    "        dict(encoding=\"latin-1\", engine=\"python\", on_bad_lines=\"skip\"),\n",
    "    ):\n",
    "        try:\n",
    "            return pd.read_csv(path, **args)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(f\"No se pudo leer {path.name} con los encodings probados.\")\n",
    "\n",
    "dfs, per_file_counts, skipped = [], [], []\n",
    "for f in files:\n",
    "    try:\n",
    "        d = read_csv_tolerant(f)\n",
    "        col_url = next((c for c in CANDIDATES if c in d.columns), None)\n",
    "        if not col_url:\n",
    "            raise KeyError(f\"{f.name} sin columna de URL reconocida {CANDIDATES}\")\n",
    "        d = d[[col_url]].rename(columns={col_url: \"url\"}).copy()\n",
    "        d[\"__source_file\"] = f.name\n",
    "        dfs.append(d)\n",
    "        per_file_counts.append((f.name, len(d)))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Saltando {f.name}: {e}\")\n",
    "        skipped.append((f.name, str(e)))\n",
    "\n",
    "assert dfs, \"Ningún CSV válido. Revisa los [WARN] y corrige.\"\n",
    "df_raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Archivos leídos:\")\n",
    "for n, k in per_file_counts:\n",
    "    print(f\" - {n}: {k} filas\")\n",
    "print(\"TOTAL filas crudas:\", len(df_raw))\n",
    "df_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf0774e-2559-4dca-8396-6818560a0134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas tras limpieza: 1111\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>__source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.caixabank.es/particular/home/parti...</td>\n",
       "      <td>banca_legitimas.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.caixabank.es/particular/general/co...</td>\n",
       "      <td>banca_legitimas.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://uniblog.unicajabanco.es</td>\n",
       "      <td>banca_legitimas.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.unicajabanco.es/es/particulares/cu...</td>\n",
       "      <td>banca_legitimas.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.unicajabanco.es/es/particulares/cu...</td>\n",
       "      <td>banca_legitimas.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        __source_file\n",
       "0  https://www.caixabank.es/particular/home/parti...  banca_legitimas.csv\n",
       "1  https://www.caixabank.es/particular/general/co...  banca_legitimas.csv\n",
       "2                    https://uniblog.unicajabanco.es  banca_legitimas.csv\n",
       "3  https://www.unicajabanco.es/es/particulares/cu...  banca_legitimas.csv\n",
       "4  https://www.unicajabanco.es/es/particulares/cu...  banca_legitimas.csv"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "df[\"url\"] = df[\"url\"].map(normalizar_url)\n",
    "df = df[df[\"url\"].map(es_url_valida)]\n",
    "df = df.dropna(subset=[\"url\"])\n",
    "df = df.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "print(\"Filas tras limpieza:\", len(df))\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c78f91b-3efb-4932-9fb1-3c175775434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filas_crudas': 1129, 'filas_limpias': 1111, '%https': np.float64(99.46), 'longitud_media': np.float64(91.48)}\n",
      "Guardado en: /Users/test/Desktop/phishing-detector/data/processed/legitimas/banca/legitimas_banca_limpio.csv\n",
      "Log añadido: 2025-08-18 | limpieza_legitimas_banca | crudo=1129 | limpio=1111 | out=/Users/test/Desktop/phishing-detector/data/processed/legitimas/banca/legitimas_banca_limpio.csv\n"
     ]
    }
   ],
   "source": [
    "# Métricas\n",
    "resumen = {\n",
    "    \"filas_crudas\": len(df_raw),\n",
    "    \"filas_limpias\": len(df),\n",
    "    \"%https\": round(df[\"url\"].str.startswith(\"https://\").mean()*100, 2),\n",
    "    \"longitud_media\": round(df[\"url\"].str.len().mean(), 2),\n",
    "}\n",
    "print(resumen)\n",
    "\n",
    "# Export\n",
    "df.to_csv(OUT_FILE, index=False)\n",
    "print(\"Guardado en:\", OUT_FILE)\n",
    "\n",
    "# Log\n",
    "log_line = (\n",
    "    f\"{datetime.now():%Y-%m-%d} | limpieza_legitimas_{SECTOR} | \"\n",
    "    f\"crudo={len(df_raw)} | limpio={len(df)} | out={OUT_FILE}\\n\"\n",
    ")\n",
    "with open(REPO_ROOT / \"docs\" / \"daily_log.md\", \"a\") as f:\n",
    "    f.write(log_line)\n",
    "print(\"Log añadido:\", log_line.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
