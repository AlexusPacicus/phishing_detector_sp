# üìå Entrenamiento ‚Äì Prototipo de detecci√≥n de phishing

## √çndice
1. [Introducci√≥n](#1-introducci√≥n)  
2. [Validaci√≥n](#2-validaci√≥n)  
3. [Modelos evaluados](#3-modelos-evaluados)  
4. [Selecci√≥n de umbral](#4-selecci√≥n-de-umbral)  
5. [Resultados finales](#5-resultados-finales)  
   - [Matriz de confusi√≥n](#51-matriz-de-confusi√≥n)  
   - [Curva ROC](#52-curva-roc)  
   - [Curva Precision-Recall](#53-curva-precision-recall)  
6. [An√°lisis por campa√±as](#6-an√°lisis-por-campa√±as-matched_target)  
7. [An√°lisis por sectores](#7-an√°lisis-por-sectores-categoria)  
8. [Conclusiones](#8-conclusiones)  
   - [Fortalezas](#fortalezas)  
   - [Debilidades](#debilidades)  
   - [Importancia de features](#importancia-de-features)  
   - [Pr√≥ximos pasos](#pr√≥ximos-pasos)



## 1. Introducci√≥n

Este notebook entrena y compara varios modelos de Machine Learning para un prototipo de detecci√≥n de phishing en el contexto espa√±ol.  

- **Dataset**: 200 URLs balanceadas  
  - 100 leg√≠timas  
  - 100 phishing  
  - 10 features seleccionadas tras un EDA detallado  

- **Objetivo**: evaluar distintos algoritmos y comparar su rendimiento con validaci√≥n cruzada, dado que el dataset es peque√±o.  

- **Modelos considerados**:  
  - Regresi√≥n Log√≠stica (`LogisticRegression`)  
  - M√°quinas de Vectores de Soporte (`SVC`)  
  - Bosques Aleatorios (`RandomForestClassifier`)  
  - XGBoost (`XGBClassifier`)  

- **M√©tricas de evaluaci√≥n**:  
  - **Accuracy** ‚Üí visi√≥n general.  
  - **Precision** ‚Üí evitar falsos positivos (URLs leg√≠timas mal bloqueadas).  
  - **Recall** ‚Üí evitar falsos negativos (phishing que pasa desapercibido).  
  - **F1-score** ‚Üí equilibrio entre precision y recall.  
  - **ROC-AUC** ‚Üí capacidad global de separar phishing de leg√≠timas.  
  - **Matriz de confusi√≥n** ‚Üí an√°lisis detallado de falsos positivos y negativos.  
  - **Specificity (TNR)** ‚Üí asegurar que la mayor√≠a de URLs leg√≠timas no son clasificadas err√≥neamente como phishing.  



## 2. Validaci√≥n

Se aplic√≥ **validaci√≥n cruzada estratificada (StratifiedKFold)** para mantener el balance de clases en cada fold.

- Se probaron valores de *k* entre **5 y 10** (incluyendo 5, 6, 7, 8, 9 y 10).  
- Para cada modelo y para cada valor de *k*, se calcularon m√©tricas de rendimiento (Accuracy, Precision, Recall, F1 y ROC-AUC).  
- Posteriormente, se tom√≥ la **media de los resultados obtenidos en todos los k (5..10)**, con el objetivo de:  
  - Reducir la varianza que se producir√≠a al elegir un √∫nico valor de *k*.  
  - Obtener una estimaci√≥n m√°s estable en un dataset peque√±o (200 URLs).  

üëâ De este modo, la tabla de resultados presentada en la secci√≥n siguiente corresponde al **promedio de todas las validaciones cruzadas realizadas entre k=5 y k=10**.



## 3. Modelos evaluados

Se probaron cuatro algoritmos representativos:

- **Logistic Regression (LR)** ‚Üí modelo lineal, interpretable y r√°pido de entrenar.  
- **SVC (lineal)** ‚Üí buena opci√≥n para datasets peque√±os, separa clases con un hiperplano √≥ptimo.  
- **Random Forest (RF)** ‚Üí ensamble de √°rboles, captura relaciones no lineales.  
- **XGBoost** ‚Üí algoritmo de boosting, fuerte en datasets grandes pero propenso a sobreajuste con pocos datos.

### Resultados promedio (k=5..10)

| Modelo             | Accuracy | Precision | Recall | F1   | ROC-AUC |
|--------------------|----------|-----------|--------|------|---------|
| LogisticRegression | 0.907    | 0.922     | 0.898  | 0.906 | 0.946   |
| RandomForest       | 0.875    | 0.879     | 0.877  | 0.875 | 0.940   |
| SVC (linear)       | 0.885    | 0.889     | 0.893  | 0.886 | 0.947   |
| XGBoost            | 0.828    | 0.841     | 0.827  | 0.824 | 0.930   |

üëâ **Logistic Regression** fue el modelo m√°s equilibrado y explicable.



## 4. Selecci√≥n de umbral

Se evaluaron distintos umbrales de decisi√≥n para la regresi√≥n log√≠stica (0.50 ‚Üí 0.25, en pasos de 0.025).  
En cada caso se calcularon precisi√≥n, recall y F1 como media de validaci√≥n cruzada estratificada con *k* = 5..10.

üìà *Gr√°fica Precision/Recall/F1 vs threshold:*  

![Precision vs Recall vs F1](img/precision_recall_threshold.png)
üëâ Se seleccion√≥ **0.425** como umbral final, por maximizar el recall (~0.92) sin que la precisi√≥n se hunda (~0.87).



## 5. Resultados finales

### 5.1 Matriz de confusi√≥n
![Matriz de confusi√≥n](img/confusion_matrix.png)

### 5.2 Curva ROC
![Curva ROC](img/roc_auc.png)

### 5.3 Curva Precision-Recall
![Curva Precision-Recall](img/precision_recall_curve.png)



## 6. An√°lisis por campa√±as (`matched_target`)

![Recall por campa√±a](img/recall_por_campana.png)

üëâ El modelo detecta bien campa√±as comunes en Espa√±a (BBVA, Santander, Orange, DGT‚Ä¶) pero falla en targets poco representados:  
- CaixaBank ‚Üí recall = 0.50  
- Correos ‚Üí recall = 0.67  
- Microsoft, Outlook, WalletConnect ‚Üí recall = 0.0



## 7. An√°lisis por sectores (`categoria`)

![Recall por sector](img/recall_por_sector.png)

üëâ Sectores fuertes: banca, telecomunicaciones, p√∫blico.  
üëâ Sectores d√©biles: log√≠stica (0.86), cripto (0.80), SaaS (0.67).



## 8. Conclusiones

### Fortalezas
- Buen rendimiento global: recall ‚âà0.92, precisi√≥n ‚âà0.87.  
- Interpretabilidad alta gracias a Logistic Regression.  
- Cobertura s√≥lida en campa√±as comunes en Espa√±a.

### Debilidades
- Dataset peque√±o (200 URLs).  
- Recall bajo en SaaS, cripto y log√≠stica.  
- Recall nulo en algunos targets minoritarios.  
- Riesgo de sobreajuste y falta de validaci√≥n externa.

### Importancia de features
![Importancia de features](img/feature_importance_grouped_clean.png)

üëâ Se confirma que `tld_group`, `protocol` y `domain_entropy` son las se√±ales m√°s influyentes, mientras que `num_params` y `contains_equal` apenas aportan.

### Pr√≥ximos pasos
1. Ampliar dataset en sectores d√©biles.  
2. A√±adir nuevas features (typosquatting, fuzzy hashing).  
3. Validaci√≥n externa en feeds distintos.  
4. Ajustar umbral seg√∫n coste operativo de FPs/FNs.
