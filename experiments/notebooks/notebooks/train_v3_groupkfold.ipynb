{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluación de Modelos V3 con GroupKFold por Entidad\n",
        "\n",
        "**Objetivo del notebook:**  \n",
        "Evaluar el rendimiento de distintos modelos de clasificación (versión 3 de features) utilizando validación cruzada basada en grupos (`GroupKFold`), donde cada grupo corresponde a una `entidad`. El objetivo es simular un entorno generalista, asegurando que los datos de cada entidad no se mezclan entre folds.\n",
        "\n",
        "**Reglas del experimento:**  \n",
        "- **No recalcular features:** El notebook utiliza el dataset preprocesado en `data/interim/dataset_v3_features.csv` (features V3).\n",
        "- **No modificar ni utilizar features o scripts de la versión v2.**\n",
        "- **Validación cruzada por grupos:** Se utiliza `GroupKFold` agrupando por la columna `entidad`.\n",
        "- **Métrica principal:** El criterio de comparación será la métrica F1.\n",
        "\n",
        "**Outputs esperados:**  \n",
        "Al finalizar, se generará una tabla resumen con el rendimiento (media y desviación estándar) de cada modelo evaluado según la métrica principal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Constantes\n",
        "DATASET_PATH = \"../data/interim/dataset_v3_features.csv\"\n",
        "RANDOM_STATE = 42\n",
        "FEATURES_V3 = [\n",
        "    \"domain_complexity\",\n",
        "    \"domain_whitelist\",\n",
        "    \"trusted_token_context\",\n",
        "    \"host_entropy\",\n",
        "    \"infra_risk\",\n",
        "    \"brand_in_path\",\n",
        "    \"brand_match_flag\"\n",
        "]\n",
        "\n",
        "TARGET = \"label\"\n",
        "GROUP_KEY = \"entidad\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (482, 13)\n",
            "Columnas: ['url', 'label', 'sector', 'entidad', 'notas', 'campaign', 'domain_complexity', 'domain_whitelist', 'trusted_token_context', 'host_entropy', 'infra_risk', 'brand_in_path', 'brand_match_flag']\n",
            "\n",
            "Entidades únicas: 110\n",
            "\n",
            "Distribución de la variable target:\n",
            "label\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columnas:\", list(df.columns))\n",
        "\n",
        "print(\"\\nEntidades únicas:\", df[GROUP_KEY].nunique())\n",
        "print(\"\\nDistribución de la variable target:\")\n",
        "print(df[TARGET].value_counts(normalize=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contract checks passed.\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# 1. Verificar que todas las columnas en FEATURES_V3 existen en df\n",
        "for col in FEATURES_V3:\n",
        "    assert col in df.columns, f\"Falta la columna de feature contractual: {col}\"\n",
        "\n",
        "# 2. Verificar que la columna TARGET existe en df\n",
        "assert TARGET in df.columns, f\"Falta la columna objetivo: {TARGET}\"\n",
        "\n",
        "# 3. Verificar que la columna GROUP_KEY existe en df y no tiene nulos\n",
        "assert GROUP_KEY in df.columns, f\"Falta la columna de grupo: {GROUP_KEY}\"\n",
        "assert df[GROUP_KEY].isnull().sum() == 0, \"Existen valores nulos en la columna de grupo (entidad)\"\n",
        "\n",
        "# 4. Verificar que df[TARGET] solo contiene {0, 1}\n",
        "labels_unicos = set(df[TARGET].unique())\n",
        "assert labels_unicos <= {0, 1}, f\"La variable target contiene valores no binarios: {labels_unicos}\"\n",
        "\n",
        "print(\"Contract checks passed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Descripción del número de muestras por entidad:\n",
            "count    110.000000\n",
            "mean       4.381818\n",
            "std       10.063999\n",
            "min        1.000000\n",
            "25%        1.000000\n",
            "50%        2.000000\n",
            "75%        3.000000\n",
            "max       73.000000\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Entidades con solo 1 muestra: 50\n",
            "Entidades con 2 muestras: 30\n",
            "Entidades con 3 o más muestras: 30\n",
            "\n",
            "Sanity check por entidad completado.\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "counts = df[GROUP_KEY].value_counts()\n",
        "\n",
        "print(\"Descripción del número de muestras por entidad:\")\n",
        "print(counts.describe())\n",
        "\n",
        "n_ent_1 = (counts == 1).sum()\n",
        "n_ent_2 = (counts == 2).sum()\n",
        "n_ent_3mas = (counts >= 3).sum()\n",
        "\n",
        "print(f\"\\nEntidades con solo 1 muestra: {n_ent_1}\")\n",
        "print(f\"Entidades con 2 muestras: {n_ent_2}\")\n",
        "print(f\"Entidades con 3 o más muestras: {n_ent_3mas}\")\n",
        "\n",
        "print(\"\\nSanity check por entidad completado.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: 17 entidades en test, 97 muestras en test\n",
            "Fold 2: 22 entidades en test, 97 muestras en test\n",
            "Fold 3: 23 entidades en test, 96 muestras en test\n",
            "Fold 4: 23 entidades en test, 96 muestras en test\n",
            "Fold 5: 25 entidades en test, 96 muestras en test\n",
            "GroupKFold (k=5) configurado correctamente.\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "\n",
        "for i, (_, test_idx) in enumerate(gkf.split(df, df[TARGET], groups=df[GROUP_KEY])):\n",
        "    entidades_test = df.iloc[test_idx][GROUP_KEY].nunique()\n",
        "    muestras_test = len(test_idx)\n",
        "    print(f\"Fold {i+1}: {entidades_test} entidades en test, {muestras_test} muestras en test\")\n",
        "\n",
        "print(\"GroupKFold (k=5) configurado correctamente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelos definidos:  ['logreg', 'rf']\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "models = {\n",
        "    \"logreg\": LogisticRegression(\n",
        "        solver=\"lbfgs\",\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    \"rf\": RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        n_jobs=-1,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"Modelos definidos: \", list(models.keys()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo añadido: ['logreg', 'rf', 'linearsvc']\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "svc_clf = CalibratedClassifierCV(\n",
        "    estimator=LinearSVC(C=1.0, random_state=RANDOM_STATE),\n",
        "    method=\"sigmoid\",\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "models[\"linearsvc\"] = svc_clf\n",
        "\n",
        "print(\"Modelo añadido:\", list(models.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation completada.\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "results = {model_name: [] for model_name in models.keys()}\n",
        "\n",
        "for fold_idx, (train_idx, test_idx) in enumerate(gkf.split(df, df[TARGET], groups=df[GROUP_KEY])):\n",
        "    X_train = df.iloc[train_idx][FEATURES_V3]\n",
        "    y_train = df.iloc[train_idx][TARGET]\n",
        "    X_test = df.iloc[test_idx][FEATURES_V3]\n",
        "    y_test = df.iloc[test_idx][TARGET]\n",
        "    \n",
        "    for model_name, model in models.items():\n",
        "        clf = model.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        if hasattr(clf, \"predict_proba\"):\n",
        "            y_proba = clf.predict_proba(X_test)[:,1]\n",
        "        else:\n",
        "            y_scores = clf.decision_function(X_test)\n",
        "            y_proba = sigmoid(y_scores)\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "        # Manejo robusto de roc_auc (evita error si sólo hay una clase en test)\n",
        "        try:\n",
        "            roc = roc_auc_score(y_test, y_proba)\n",
        "        except Exception:\n",
        "            roc = float('nan')\n",
        "        results[model_name].append({\n",
        "            'fold': fold_idx+1,\n",
        "            'precision': prec,\n",
        "            'recall': rec,\n",
        "            'f1': f1,\n",
        "            'roc_auc': roc\n",
        "        })\n",
        "\n",
        "print(\"Cross-validation completada.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       model  precision_mean  precision_std  recall_mean  recall_std  \\\n",
            "0  linearsvc        0.893825       0.072115     0.921723    0.053646   \n",
            "1         rf        0.916678       0.030643     0.891112    0.089996   \n",
            "2     logreg        0.883610       0.072299     0.926786    0.064648   \n",
            "\n",
            "    f1_mean    f1_std  roc_auc_mean  roc_auc_std  \n",
            "0  0.906085  0.054124      0.966365     0.022264  \n",
            "1  0.902496  0.061998      0.957696     0.024732  \n",
            "2  0.902171  0.052126      0.963844     0.024387  \n",
            "Resumen de métricas generado.\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "rows = []\n",
        "for model_name, folds in results.items():\n",
        "    precs = [fold['precision'] for fold in folds]\n",
        "    recs = [fold['recall'] for fold in folds]\n",
        "    f1s = [fold['f1'] for fold in folds]\n",
        "    rocs = [fold['roc_auc'] for fold in folds]\n",
        "    rows.append({\n",
        "        \"model\": model_name,\n",
        "        \"precision_mean\": np.mean(precs),\n",
        "        \"precision_std\": np.std(precs),\n",
        "        \"recall_mean\": np.mean(recs),\n",
        "        \"recall_std\": np.std(recs),\n",
        "        \"f1_mean\": np.mean(f1s),\n",
        "        \"f1_std\": np.std(f1s),\n",
        "        \"roc_auc_mean\": np.nanmean(rocs),  # ignora NaN\n",
        "        \"roc_auc_std\": np.nanstd(rocs)     # ignora NaN\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(rows)[[\n",
        "    \"model\",\n",
        "    \"precision_mean\", \"precision_std\",\n",
        "    \"recall_mean\", \"recall_std\",\n",
        "    \"f1_mean\", \"f1_std\",\n",
        "    \"roc_auc_mean\", \"roc_auc_std\"\n",
        "]]\n",
        "summary_df = summary_df.sort_values(by=\"f1_mean\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(summary_df)\n",
        "print(\"Resumen de métricas generado.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo ganador: linearsvc\n",
            "       model  precision_mean  precision_std  recall_mean  recall_std  \\\n",
            "0  linearsvc        0.893825       0.072115     0.921723    0.053646   \n",
            "\n",
            "    f1_mean    f1_std  roc_auc_mean  roc_auc_std  \n",
            "0  0.906085  0.054124      0.966365     0.022264  \n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "df_sorted = summary_df.sort_values(by=\"f1_mean\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "if len(df_sorted) > 1 and (df_sorted.iloc[0][\"f1_mean\"] - df_sorted.iloc[1][\"f1_mean\"]) < 0.002:\n",
        "    top = df_sorted[df_sorted[\"f1_mean\"] == df_sorted.iloc[0][\"f1_mean\"]]\n",
        "    if len(top) > 1:\n",
        "        top = top.sort_values(by=\"roc_auc_mean\", ascending=False).reset_index(drop=True)\n",
        "        if len(top) > 1 and top.iloc[0][\"roc_auc_mean\"] == top.iloc[1][\"roc_auc_mean\"]:\n",
        "            top = top.sort_values(by=\"recall_mean\", ascending=False).reset_index(drop=True)\n",
        "    best_model_name = top.iloc[0][\"model\"]\n",
        "else:\n",
        "    best_model_name = df_sorted.iloc[0][\"model\"]\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(\"Modelo ganador:\", best_model_name)\n",
        "print(summary_df.loc[summary_df[\"model\"] == best_model_name])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo re-entrenado para análisis (no exportado).\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Entrenamiento EXPLORATORIO del modelo ganador (NO contractual)\n",
        "# Solo para análisis posterior (curvas, probas, threshold tuning)\n",
        "\n",
        "X = df[FEATURES_V3]\n",
        "y = df[TARGET]\n",
        "\n",
        "best_model.fit(X, y)\n",
        "\n",
        "print(\"Modelo re-entrenado para análisis (no exportado).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusiones de la evaluación por entidad (GroupKFold)\n",
        "\n",
        "Tras la evaluación con validación cruzada por entidad, el modelo `linearsvc` ha sido seleccionado como el ganador. La decisión se basa en que obtiene el mejor valor de F1_mean y de ROC-AUC_mean, mostrando además un recall competitivo y un buen equilibrio entre todas las métricas principales. Considera también la menor varianza relativa de sus métricas frente a Random Forest (RF), lo que aporta una mayor robustez en escenarios con entidades disjuntas.\n",
        "\n",
        "Desde una perspectiva técnica, la estabilidad de las métricas a nivel de entidad es crucial en detección de phishing enfocado a España, ya que permite una generalización más efectiva y minimiza el riesgo de sobreajuste a entidades individuales, mejorando la detección en entidades desconocidas o futuras.\n",
        "\n",
        "Por todo ello, consideramos este modelo como baseline v3 listo para la fase de exportación y posterior integración.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (phishing-env)",
      "language": "python",
      "name": "phishing-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
