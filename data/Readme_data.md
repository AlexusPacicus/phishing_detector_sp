# ESTRUCTURA DE LA CARPETA "DATA"

Aqu√≠ se encuentran los archivos de datos usados en el proyecto.  
Se subdivide en "processed" (los datos ya procesados) y "raw" (datos originales).

## Raw 
Contiene los archivos **originales**, sin modificar.  
Incluye datasets descargados autom√°ticamente (feeds globales y nacionales), datos recolectados por scraping o recopilaci√≥n manual, y logs de los procesos de descarga.

### üìë **Fuentes de datos de phishing**

#### **URLhaus**
- **Fuente:** [URLhaus](https://urlhaus.abuse.ch/)
- **Script:** `auto_urlhaus_csv_online.py`
- **Formato:** CSV (`csv_online`), solo URLs activas
- **Columnas extra√≠das:**
  - `url`: direcci√≥n maliciosa
  - `dateadded`: fecha de inclusi√≥n
  - `threat`: tipo de amenaza
- **Notas:**
  - Se descarta la API por bloqueos a IPs espa√±olas y el feed completo ZIP por dificultad de procesamiento.
  - El feed ya viene filtrado por URLs activas, lo que facilita el uso.
- **Problemas resueltos:** Bloqueos por IP en la API, formato ZIP inconsistente en el feed completo.

#### **PhishTank**
- **Fuente:** [PhishTank](https://phishtank.org/)
- **Script:** `aut_phishtank.py`
- **Formato:** CSV convertido desde XML o JSON
- **Columnas extra√≠das:**
  - `url`: direcci√≥n sospechosa
  - `submission_time`: fecha de reporte
  - `verified`, `verification_time`: verificaci√≥n por la comunidad
  - `online`: si la URL sigue activa
  - `target`: objetivo del phishing (si est√° disponible)
- **Notas:**
  - Se filtra para quedarse solo con URLs `online` y con `verified=True`.
  - Puede haber diferencias en el formato seg√∫n la versi√≥n del feed.
  - **Mejora recomendada:** Renombrar `submission_time` como `dateadded` para homogeneizar con el resto de fuentes.

#### **OpenPhish**
- **Fuente:** [OpenPhish](https://openphish.com/)
- **Script:** `automatizacion_openphish.py`
- **Formato:** TXT (una URL por l√≠nea) o CSV en versi√≥n premium
- **Columnas extra√≠das:**
  - `url`: direcci√≥n detectada como phishing
  - (opcional) `date`: fecha de detecci√≥n
  - (opcional) `threat`: tipo de amenaza
- **Notas:**
  - El feed gratuito solo da la URL, sin m√°s metadatos.
  - **Mejora recomendada:** Si solo hay `url`, a√±ade columna `source` con `"OpenPhish"` y la fecha de descarga como `dateadded`.

**Todos los feeds se almacenan individualmente en `/raw/phishing/` para trazabilidad, control de calidad y futura auditor√≠a.**  
En la fase de preprocesado, los datasets son fusionados, deduplicados y normalizados, guardando siempre la columna `source` para saber su origen.

---

## Processed

Archivos de datos ya filtrados, fusionados o transformados, listos para su an√°lisis o entrenamiento.

### **Ejemplo de archivos:**
- `processed/phishing_clean.csv` ‚Äì dataset combinado, deduplicado y filtrado para entrenamiento del modelo.
- `processed/legit_urls_filtered.csv` ‚Äì dataset de URLs leg√≠timas, tras limpieza y verificaci√≥n.

---

## Datasets principales

### Dataset Phishing  
- Fuentes: URLhaus, PhishTank, OpenPhish.
- M√©todos: Descarga autom√°tica con scripts en `/scripts/`, fusi√≥n y limpieza en `/notebooks/`.
- Nota: Para detalles t√©cnicos, consulta el README de cada fuente en `/data/raw/phishing/` (si lo creas).

### URLs leg√≠timas  
- Fuentes: Principales bancos espa√±oles, scraping manual, OSINT.
- M√©todos: Navegaci√≥n manual, Google Dorks, validaci√≥n manual, exclusi√≥n de rutas no relevantes.

---

---
## Cambios recientes y mejoras en la recogida de datos (28/07/2025)

- A√±adido control de duplicados (`drop_duplicates(subset='url')`) antes de guardar en todos los scripts de feeds (URLhaus, OpenPhish, PhishTank).
- Mejorado el logging: ahora cada fase (descarga, limpieza, guardado) registra intentos, errores, n√∫mero de URLs, duplicados eliminados, fuente y ruta de archivos.
- Modularizaci√≥n y docstrings en todas las funciones principales para m√°xima claridad y mantenibilidad.
- Todos los scripts a√±aden las columnas `fuente` y `fecha_hora_recoleccion` para trazabilidad completa.
- El script de PhishTank est√° preparado y documenta errores correctamente, aunque la descarga autom√°tica no es posible actualmente por limitaciones de acceso externas.
- Documentaci√≥n y ejemplos actualizados para reflejar estas mejoras.

## Pr√≥ximos pasos

- Ampliar el dataset leg√≠timo con m√°s sectores y fuentes.
- Integrar todos los datos en el pipeline de preprocesamiento y modelado.
- Documentar el proceso completo en notebooks y scripts, y mantener trazabilidad de los cambios y decisiones tomadas.

---

*Para m√°s detalles sobre el pipeline y la estructura global, consulta el README principal del proyecto.*
