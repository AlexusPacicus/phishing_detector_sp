{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9692ee2c-0131-44dd-b0d1-679feb2b06fb",
   "metadata": {},
   "source": [
    "# Scraping de URLs Legítimas – Sector SaaS / Cloud / Correo\n",
    "\n",
    "## 1. Objetivo\n",
    "\n",
    "Documentar el proceso de recopilación de URLs legítimas de los principales servicios SaaS, cloud y correo electrónico, para construir el dataset de entrenamiento del modelo de detección de phishing.\n",
    "\n",
    "## 2. Lista de servicios incluidos y resultados\n",
    "\n",
    "| Empresa       | URLs únicas | Fecha      | Observaciones principales                      |\n",
    "|---------------|------------|------------|-----------------------------------------------|\n",
    "| Box           | 30         | 22/07/2025 | Muchos enlaces internos en home, pero mezcla de rutas (login, ayuda, TOS, etc.) |\n",
    "| Slack         | 28         | 22/07/2025 | Muchos enlaces internos en home, mezcla de rutas relevantes y otras auxiliares   |\n",
    "| Google        | 4          | 22/07/2025 | Muy pocos enlaces, home minimalista           |\n",
    "| Zoom          | 3          | 22/07/2025 | Muy pocos enlaces, home minimalista           |\n",
    "| Fastmail      | 2          | 22/07/2025 | Home minimalista, pocos enlaces               |\n",
    "| OVHcloud      | 2          | 22/07/2025 | Home minimalista, pocos enlaces               |\n",
    "| Mega          | 1          | 22/07/2025 | Solo la URL principal obtenida                |\n",
    "| Yandex Mail   | 1          | 22/07/2025 | Solo la URL principal obtenida                |\n",
    "| iCloud        | 1          | 22/07/2025 | Solo la URL principal obtenida                |\n",
    "\n",
    "  |\n",
    "\n",
    "## 3. Métodos y herramientas empleadas\n",
    "\n",
    "- **Scraping básico** realizado con `requests` y `BeautifulSoup`.\n",
    "- Solo se han extraído URLs directamente visibles (home, login, recuperación, área de acceso).\n",
    "- **No se ha hecho limpieza de datos, crawling, uso de palabras clave, ni Selenium** en esta fase.\n",
    "- Las URLs pueden contener enlaces poco relevantes y necesitan revisión manual en fases posteriores.\n",
    "\n",
    "## 4. Datos obtenidos\n",
    "\n",
    "- Total de URLs recopiladas: **72**\n",
    "- URLs únicas tras limpieza: **72** (sin duplicados exactos)\n",
    "- Ejemplo de datos:\n",
    "\n",
    "    | Empresa   | URL relevante                                                |\n",
    "    |-----------|-------------------------------------------------------------|\n",
    "    | Box       | https://account.box.com/login                                |\n",
    "    | Slack     | https://slack.com/signin                                     |\n",
    "    | Google    | https://accounts.google.com/signin/usernamerecovery          |\n",
    "    | Zoom      | https://zoom.us/signin#nested                                |\n",
    "\n",
    "- Guardado en: `data/raw/saas_legitimas_crudo.csv`\n",
    "\n",
    "## 5. Problemas encontrados y soluciones\n",
    "\n",
    "- **Desbalance**: Box y Slack aportan la mayoría de URLs; el resto de empresas tienen muy pocos resultados.\n",
    "- Varias webs presentan homes minimalistas y apenas enlazan rutas internas útiles.\n",
    "- No hay limpieza ni filtrado de URLs poco relevantes todavía (pendiente).\n",
    "- Algunas webs pueden requerir técnicas de scraping avanzado (Selenium) para extraer rutas adicionales.\n",
    "\n",
    "## 6. Lecciones aprendidas y recomendaciones\n",
    "\n",
    "- El scraping básico solo es eficaz en webs con muchas rutas internas públicas.\n",
    "- Para mejorar calidad y cantidad de datos:\n",
    "  - Filtrar por palabras clave (“login”, “signin”, “reset”, etc.).\n",
    "  - Implementar crawling más profundo.\n",
    "  - Probar Selenium en webs con protección o contenido dinámico.\n",
    "- Documentar siempre limitaciones de cada fase antes de iterar.\n",
    "\n",
    "## 7. Próximos pasos\n",
    "\n",
    "- Implementar filtrado y limpieza de URLs (palabras clave relevantes).\n",
    "- Ampliar crawling y probar Selenium donde sea necesario.\n",
    "- Revisar empresas con baja cobertura aplicando nuevas técnicas en el futuro.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phishing-env)",
   "language": "python",
   "name": "phishing-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
