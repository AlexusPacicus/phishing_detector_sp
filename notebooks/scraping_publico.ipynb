{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0726d881-d6e2-4b24-9388-c9743d6ed656",
   "metadata": {},
   "source": [
    "## Sector Público: Scraping de URLs\n",
    "\n",
    "**Fecha de ejecución:** 22/07/2025  \n",
    "**Empresas objetivo:** Agencia Tributaria (AEAT), Seguridad Social, DGT, SEPE, INEM, Suma Gestión Tributaria, Catastro\n",
    "\n",
    " | Empresa                    | URLs únicas | Fecha      | Estado / Observaciones           |\n",
    "|----------------------------|------------|------------|----------------------------------|\n",
    "| DGT                        |    287     | 22/07/2025 | OK                               |\n",
    "| SEPE                       |    105     | 22/07/2025 | OK                               |\n",
    "| Agencia Tributaria (AEAT)  |    101     | 22/07/2025 | OK                               |\n",
    "| Seguridad Social           |     74     | 22/07/2025 | OK                               |\n",
    "| Suma Gestión Tributaria    |     59     | 22/07/2025 | OK                               |\n",
    "| Catastro                   |     21     | 22/07/2025 | OK                               |\n",
    "| INEM                       |      1     | 22/07/2025 | OK – Muy pocos enlaces           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a6cc4e-a18a-488b-8f75-120100018838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias para el scraping y el análisis de datos\n",
    "import requests                             # Para hacer peticiones HTTP a las webs\n",
    "from bs4 import BeautifulSoup               # Para parsear el HTML de la página web\n",
    "from urllib.parse import urlparse, urljoin  # Para trabajar con URLs y dominios\n",
    "import pandas as pd                         # Para almacenar y exportar los datos en DataFrame\n",
    "import time                                 # Para pausar entre peticiones y evitar bloqueos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614b5b25-c245-410f-9620-a0bd81e3631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_urls(base_url, delay=1):\n",
    "    \"\"\"\n",
    "    Descarga la página principal de una web y extrae todas las URLs internas.\n",
    "    Solo añade URLs que pertenezcan al dominio principal o subdominios de la empresa.\n",
    "    - base_url: URL principal de la empresa\n",
    "    - delay: segundos de espera tras cada request para evitar bloqueos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Definimos un User-Agent \"realista\" para que el servidor no bloquee la petición por bot\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        # Hacemos la petición HTTP a la web (con timeout de 10 segundos)\n",
    "        response = requests.get(base_url, timeout=10, headers=headers)\n",
    "\n",
    "        # Parseamos el HTML recibido con BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extraemos el dominio principal (ej: 'amazon.es' de 'www.amazon.es')\n",
    "        dominio_empresa = urlparse(base_url).netloc\n",
    "        if dominio_empresa.startswith('www.'):\n",
    "            dominio_base = dominio_empresa[4:]\n",
    "        else:\n",
    "            dominio_base = dominio_empresa\n",
    "\n",
    "        # Creamos un set para guardar solo URLs únicas\n",
    "        urls = set()\n",
    "\n",
    "        # Buscamos todos los enlaces <a> que tengan atributo href\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']                  # Obtenemos el href\n",
    "            href = urljoin(base_url, href)       # Convertimos a URL absoluta\n",
    "            dominio_href = urlparse(href).netloc # Dominio del enlace\n",
    "\n",
    "            # Eliminamos 'www.' para comparar dominios correctamente\n",
    "            if dominio_href.startswith('www.'):\n",
    "                dominio_href_base = dominio_href[4:]\n",
    "            else:\n",
    "                dominio_href_base = dominio_href\n",
    "\n",
    "            # Añadimos solo si es del dominio principal o subdominio\n",
    "            if dominio_href_base == dominio_base or dominio_href_base.endswith('.' + dominio_base):\n",
    "                urls.add(href)\n",
    "\n",
    "        # Esperamos el tiempo indicado (para no ser bloqueados por el servidor)\n",
    "        time.sleep(delay)\n",
    "\n",
    "        # Devolvemos la lista de URLs y un estado \"OK\"\n",
    "        return list(urls), \"OK\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si hay error, lo devolvemos como estado y lista vacía de URLs\n",
    "        return [], str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91bc53f-bb93-4ce0-aa89-0fccb915c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_sector_publico = {\n",
    "    \"Agencia Tributaria (AEAT)\": \"https://www.agenciatributaria.es\",\n",
    "    \"Seguridad Social\": \"https://www.seg-social.es\",\n",
    "    \"DGT\": \"https://www.dgt.es\",\n",
    "    \"SEPE\": \"https://www.sepe.es\",\n",
    "    \"INEM\": \"https://www.inem.es\",\n",
    "    \"Suma Gestión Tributaria\": \"https://www.suma.es\",\n",
    "    \"Catastro\": \"https://www.sedecatastro.gob.es\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d7d1f-bf31-4e81-99a6-2fe856007dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4459a367-381d-477e-9bd3-a93cfde42ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping empresa: Agencia Tributaria (AEAT)\n",
      "Scraping empresa: Seguridad Social\n",
      "Scraping empresa: DGT\n",
      "Scraping empresa: SEPE\n",
      "Scraping empresa: INEM\n",
      "Scraping empresa: Suma Gestión Tributaria\n",
      "Scraping empresa: Catastro\n",
      "                     empresa  \\\n",
      "0  Agencia Tributaria (AEAT)   \n",
      "1  Agencia Tributaria (AEAT)   \n",
      "2  Agencia Tributaria (AEAT)   \n",
      "3  Agencia Tributaria (AEAT)   \n",
      "4  Agencia Tributaria (AEAT)   \n",
      "\n",
      "                                                 url       fecha estado  \n",
      "0  https://www.agenciatributaria.es/Sede/informac...  22/07/2025     OK  \n",
      "1  https://www.agenciatributaria.es/Sede/identifi...  22/07/2025     OK  \n",
      "2  https://www.agenciatributaria.es/Sede/estadist...  22/07/2025     OK  \n",
      "3  https://www.agenciatributaria.es/Sede/ayuda/di...  22/07/2025     OK  \n",
      "4  https://www.agenciatributaria.es/Sede/certific...  22/07/2025     OK  \n"
     ]
    }
   ],
   "source": [
    "# Lista donde almacenaremos los resultados de todas las empresas\n",
    "resultados = []\n",
    "\n",
    "# Guardamos la fecha actual\n",
    "fecha = pd.Timestamp.today().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# Iteramos sobre cada empresa y su URL\n",
    "for nombre, url_base in empresas_sector_publico.items():\n",
    "    print(f'Scraping empresa: {nombre}')                # Log para saber el progreso\n",
    "    urls, estado = obtener_urls(url_base)               # Llamamos a la función de scraping\n",
    "    if urls:\n",
    "        # Si hay URLs, añadimos cada una como fila en los resultados\n",
    "        for url in urls:\n",
    "            resultados.append({\n",
    "                'empresa': nombre,\n",
    "                'url': url,\n",
    "                'fecha': fecha,\n",
    "                'estado': estado\n",
    "            })\n",
    "    else:\n",
    "        # Si falla o no hay URLs, registramos igualmente el intento\n",
    "        resultados.append({\n",
    "            'empresa': nombre,\n",
    "            'url': '',\n",
    "            'fecha': fecha,\n",
    "            'estado': estado\n",
    "        })\n",
    "\n",
    "# Convertimos la lista de resultados en un DataFrame de pandas\n",
    "df_scrap = pd.DataFrame(resultados)\n",
    "\n",
    "# Guardamos el DataFrame como CSV en la carpeta correspondiente (cambia el nombre por sector)\n",
    "df_scrap.to_csv('../data/raw/publico_legitimas_crudo.csv', index=False)\n",
    "\n",
    "# Mostramos por pantalla las primeras filas del DataFrame para revisión rápida\n",
    "print(df_scrap.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268581f-e9c5-4ce6-9275-47b6f594a3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d90b4e-36db-4d01-902b-b080dd31d8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4615d3f9-d15d-4e54-82b8-69bc877a2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empresa\n",
      "DGT                          287\n",
      "SEPE                         105\n",
      "Agencia Tributaria (AEAT)    101\n",
      "Seguridad Social              74\n",
      "Suma Gestión Tributaria       59\n",
      "Catastro                      21\n",
      "INEM                           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_scrap['empresa'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dffd33-21e3-4d83-a860-c30db3412aeb",
   "metadata": {},
   "source": [
    "### Resumen y análisis del scraping (Sector Público)\n",
    "\n",
    "- **Empresas scrapeadas:** 7\n",
    "- **Total URLs obtenidas:** 648\n",
    "- **Patrones detectados:**\n",
    "    - DGT y SEPE destacan con alto número de URLs públicas.\n",
    "    - INEM y Catastro apenas muestran enlaces (probable home muy básica o minimalista).\n",
    "- **Incidencias:**\n",
    "    - Ningún bloqueo aparente ni errores críticos; la mayoría de portales permiten scraping simple.\n",
    "- **Siguientes pasos:**\n",
    "    - Si interesa, scrapear secciones internas en empresas con pocos enlaces (INEM, Catastro).\n",
    "    - Continuar con el siguiente sector.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phishing-env)",
   "language": "python",
   "name": "phishing-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
